{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text as Data Coursework - 2385679b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 - Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell below counts and sorts labels. Is incorrect as doesn't count inline instructions which will increase total samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common labels:\n",
      "instruction:  414\n",
      "Wallace:  369\n",
      "Lady Tottington:  96\n",
      "Victor:  84\n",
      "Piella:  63\n",
      "Wendolene:  35\n",
      "Reverend Hedges:  25\n",
      "PC Mackintosh:  17\n",
      "Mrs. Mulch:  17\n",
      "Hutch:  16\n",
      "\n",
      "total samples 1225\n",
      "\n",
      "total samples of 10 most common labels:  1136\n",
      "\n",
      "number of labels:  34\n",
      "\n",
      "labels: \n",
      " {'instruction': 414, 'PC Mackintosh': 17, 'Wallace': 369, 'Mr. Dibber': 5, 'Mrs. Girdling': 5, 'Mrs. Mulch': 17, 'Mr. Mulch': 3, 'Rabbit': 3, 'Reverend Hedges': 25, 'Rabbits': 6, 'Lady Tottington': 96, 'Victor': 84, 'Philip': 0, 'Mr. Caliche': 5, 'Were-Rabbit': 2, 'Miss Blight': 2, 'Mr. Windfall': 4, 'Miss Thripp': 4, 'Mr. Crock': 1, 'Mr. Growbag': 6, 'Man': 8, 'Woman': 1, 'Hutch': 16, 'Man 1': 0, 'Man 2': 0, 'Man 3': 0, 'Woman 1': 0, 'Man 4': 0, ' Hutch': 0, 'All': 1, ' Philip': 0, 'Mr. Leaching': 0, 'Wendolene': 35, 'Piella': 63}\n"
     ]
    }
   ],
   "source": [
    "f = open(\"new_script.txt\", \"r\")\n",
    "number_of_samples = 0\n",
    "labels = {\"instruction\": 0}\n",
    "for line in f:\n",
    "    # Go through each line of the file\n",
    "    if line[0] == \"*\" or line[0] == \"[\" or line[0] == \"'\":\n",
    "        number_of_samples +=1\n",
    "        apostraphies = 0\n",
    "        index = 0\n",
    "        speaker = \"\"\n",
    "        \n",
    "        # Extract the label\n",
    "        while apostraphies < 6 and index < len(line):\n",
    "            character = line[index]\n",
    "            if character == \"[\":\n",
    "                speaker = \"instruction\"\n",
    "                break\n",
    "            elif character == \":\":\n",
    "                break\n",
    "            elif character == \"'\":\n",
    "                apostraphies += 1\n",
    "            elif character != \"*\":\n",
    "                speaker += character\n",
    "            index += 1\n",
    "            \n",
    "        # Add to the label's count or add the label if new\n",
    "        if speaker in labels:\n",
    "            labels[speaker] += 1\n",
    "        else:\n",
    "            labels[speaker] = 0\n",
    "\n",
    "lables_copy = labels.copy()\n",
    "tot_val = 0\n",
    "\n",
    "# Print out label information\n",
    "print(\"10 most common labels:\")\n",
    "\n",
    "# Get 10 most frequent labels\n",
    "for i in range(10):\n",
    "    largest_val = 0\n",
    "    largest_item = \"\"\n",
    "    for item in lables_copy:\n",
    "        if lables_copy[item] > largest_val:\n",
    "            largest_val = lables_copy[item]\n",
    "            largest_item = item\n",
    "    print(largest_item + \": \", largest_val)\n",
    "    lables_copy.pop(largest_item)\n",
    "    tot_val += largest_val\n",
    "\n",
    "\n",
    "print(\"\\ntotal samples\", number_of_samples)\n",
    "print(\"\\ntotal samples of 10 most common labels: \", tot_val)\n",
    "print(\"\\nnumber of labels: \", len(labels))\n",
    "print(\"\\nlabels: \\n\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell below Creates the actual dictionary of samples with their respective labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"new_script.txt\", \"r\")\n",
    "all_samples = []\n",
    "for line in f:\n",
    "    index = 0\n",
    "    if line[index] == \"*\" or line[index] == \"[\":\n",
    "        semicolon = False\n",
    "        semicolon_index = 0\n",
    "        sample = \"\"\n",
    "        label = \"\"\n",
    "        instruction_label = \"instruction\"\n",
    "            \n",
    "        # Check if there is a semicolon, as then it is a characer's line\n",
    "        # At the same time create the label of that character\n",
    "        while semicolon_index < len(line):\n",
    "            if line[semicolon_index] == \":\":\n",
    "                semicolon = True\n",
    "                break\n",
    "            else:\n",
    "                if line[semicolon_index] != \"'\" and line[semicolon_index] != \"*\":\n",
    "                    label += line[semicolon_index]\n",
    "                semicolon_index += 1\n",
    "\n",
    "        # If it is a character, extract their line to create the sample\n",
    "        if semicolon:\n",
    "            index = semicolon_index + 1\n",
    "            while index < len(line):\n",
    "                \n",
    "                # check for, and extract, in-line instruction\n",
    "                if line[index] == \"[\":\n",
    "                    instruction_sample = \"\"\n",
    "                    while line[index] != \"]\":\n",
    "                        if line[index] == \"'\" and line[index+1] == \"'\":\n",
    "                            index += 2\n",
    "                            continue\n",
    "                        else:\n",
    "                            instruction_sample += line[index]\n",
    "                            index += 1\n",
    "                    all_samples.append([instruction_label, instruction_sample])\n",
    "                    index += 1 \n",
    "                    \n",
    "                # If not part of an instruction\n",
    "                else:\n",
    "                    sample += line[index]\n",
    "                    index += 1\n",
    "                \n",
    "            # Reached eol, add sample to dictionary\n",
    "            all_samples.append([label, sample])\n",
    "                \n",
    "        # If it is not a character's line, it is an instruction\n",
    "        else:\n",
    "            label = \"instruction\"\n",
    "            while index < len(line):\n",
    "                if line[index] == \"]\":\n",
    "                    break\n",
    "                elif line[index] == \"'\" and line[index+1] == \"'\":\n",
    "                    index += 2\n",
    "                    continue\n",
    "                else:\n",
    "                    if line[index] != \"[\":\n",
    "                        sample += line[index]\n",
    "                    index += 1\n",
    "                \n",
    "            all_samples.append([label, sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up odd values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up samples\n",
    "for sample in all_samples:\n",
    "    sample[1] = sample[1].replace(\"Ã‚\", \"\")\n",
    "    sample[1] = sample[1].replace(\"[\", \"\")\n",
    "    sample[1] = sample[1].replace(\"  \", \" \")\n",
    "    sample[1] = sample[1].replace(\"*\", \"\")\n",
    "    sample[1] = sample[1].replace(\"\\n\", \"\")\n",
    "    sample[1] = sample[1].replace(\"\\xa0\", \"\")\n",
    "    sample[1] = sample[1].replace(\"'''\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with label:count pairs so we can get the most common after cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with label:count pairs\n",
    "label_dict = {}\n",
    "for sample in all_samples:\n",
    "    if sample[0] in label_dict:\n",
    "        label_dict[sample[0]] += 1\n",
    "    else:\n",
    "        label_dict[sample[0]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a final dictionary with the five most common labels and their samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction:  847\n",
      "Wallace:  370\n",
      "Lady Tottington:  97\n",
      "Victor:  85\n",
      "Piella:  64\n",
      "total values:  1463\n",
      "top five labels:  ['instruction', 'Wallace', 'Lady Tottington', 'Victor', 'Piella'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the five most common labels and their assiociated lines\n",
    "tot_val = 0\n",
    "top_five = []\n",
    "for i in range(5):\n",
    "    largest_val = 0\n",
    "    largest_item = \"\"\n",
    "    for item in label_dict:\n",
    "        if label_dict[item] > largest_val:\n",
    "            largest_val = label_dict[item]\n",
    "            largest_item = item\n",
    "    top_five.append(largest_item)\n",
    "    print(largest_item + \": \", largest_val)\n",
    "    label_dict.pop(largest_item)\n",
    "    tot_val += largest_val\n",
    "print(\"total values: \", tot_val)\n",
    "print(\"top five labels: \", top_five, \"\\n\")\n",
    "\n",
    "final_set = []\n",
    "for sample in all_samples:\n",
    "    if sample[0] in top_five:\n",
    "        temp_dict = {}\n",
    "        temp_dict[sample[0]] = sample[1]\n",
    "        final_set.append(temp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the final set will be split into training validation and test sets in a 60/20/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(texts_train)=877\tlen(labels_train)=877\n",
      "len(texts_val)=293\tlen(labels_val)=293\n",
      "len(texts_test)=293\tlen(labels_test)=293\n",
      "Fluffles accientally knocks Wallace, yelling and crashing through the wall, then Piella continues attacking instruction\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "for i in final_set:\n",
    "    for key, value in i.items():\n",
    "        labels.append(key)\n",
    "        texts.append(value)\n",
    "\n",
    "texts_train_val, texts_test, labels_train_val, labels_test = train_test_split(\n",
    "    texts, labels, test_size = 0.2, random_state = 42)\n",
    "\n",
    "texts_train, texts_val, labels_train, labels_val = train_test_split(\n",
    "    texts_train_val, labels_train_val, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print(f\"{len(texts_train)=}\\t{len(labels_train)=}\")\n",
    "print(f\"{len(texts_val)=}\\t{len(labels_val)=}\")\n",
    "print(f\"{len(texts_test)=}\\t{len(labels_test)=}\")\n",
    "print(texts_train[42], labels_train[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution of labels across different sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing set:  {'Piella': 14, 'instruction': 178, 'Wallace': 68, 'Lady Tottington': 15, 'Victor': 18}\n",
      "validation set:  {'instruction': 173, 'Victor': 16, 'Wallace': 71, 'Lady Tottington': 21, 'Piella': 12}\n",
      "training set:  {'Victor': 51, 'Wallace': 231, 'instruction': 496, 'Piella': 38, 'Lady Tottington': 61}\n"
     ]
    }
   ],
   "source": [
    "label_dict_train = {}\n",
    "label_dict_val = {}\n",
    "label_dict_test = {}\n",
    "for label in labels_train:\n",
    "    if label in label_dict_train:\n",
    "        label_dict_train[label] += 1\n",
    "    else:\n",
    "        label_dict_train[label] = 1\n",
    "\n",
    "for label in labels_val:\n",
    "    if label in label_dict_val:\n",
    "        label_dict_val[label] += 1\n",
    "    else:\n",
    "        label_dict_val[label] = 1\n",
    "\n",
    "for label in labels_test:\n",
    "    if label in label_dict_test:\n",
    "        label_dict_test[label] += 1\n",
    "    else:\n",
    "        label_dict_test[label] = 1\n",
    "        \n",
    "print(\"testing set: \", label_dict_test)\n",
    "print(\"validation set: \", label_dict_val)\n",
    "print(\"training set: \", label_dict_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0 - Vectorize text\n",
    "\n",
    "\n",
    "This will require:\n",
    " * Tokenizing the text\n",
    " * Creating a vocabulary\n",
    " * Get document frequency\n",
    " * Create tf-idf sparse vectors\n",
    " * Normalize the vectors\n",
    " \n",
    "I did all this using functions created during the lab. However, I then also did the same process using scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\Users\\trbal\\anaconda3\\lib\\site-packages\\torch\\lib\\caffe2_nvrtc.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_pipeline_spacy\u001b[39m(text):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\thinc\\api.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, registry, ConfigValidationError\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normal_init, uniform_init, glorot_uniform_init, zero_init\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m configure_normal_init\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CategoricalCrossentropy, L2Distance, CosineDistance\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\thinc\\initializers.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, cast\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ops\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloatsXd, Shape\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\thinc\\backends\\__init__.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontextvars\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContextVar\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ops\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcupy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CupyOps, has_cupy\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\thinc\\backends\\ops.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloatsXd, Ints1d, Ints2d, Ints3d, Ints4d, IntsXd, _Floats\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeviceTypes, Generator, Padded, Batchable, SizedGenerator\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_array_module, is_xp_array, to_numpy\n\u001b[0;32m     16\u001b[0m ArrayT \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArrayT\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mArrayXd)\n\u001b[0;32m     17\u001b[0m FloatsT \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFloatsT\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39m_Floats)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\thinc\\util.py:29\u001b[0m\n\u001b[0;32m     25\u001b[0m     has_cupy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdlpack\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\__init__.py:139\u001b[0m\n\u001b[0;32m    137\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    138\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 139\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    141\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m():\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\trbal\\anaconda3\\lib\\site-packages\\torch\\lib\\caffe2_nvrtc.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def text_pipeline_spacy(text):\n",
    "    tokens = []\n",
    "    doc = nlp(text)\n",
    "    for t in doc:\n",
    "        if not t.is_stop and not t.is_punct and not t.is_space:\n",
    "            tokens.append(t.lemma_.lower())\n",
    "    return tokens\n",
    "\n",
    "texts_train_copy = texts_train.copy()\n",
    "texts_val_copy = texts_val.copy()\n",
    "texts_test_copy = texts_test.copy()\n",
    "\n",
    "texts_train_tokens = []\n",
    "texts_val_tokens = []\n",
    "texts_test_tokens = []\n",
    "\n",
    "for line in texts_train_copy:\n",
    "    texts_train_tokens.append(text_pipeline_spacy(line))\n",
    "    \n",
    "for line in texts_val_copy:\n",
    "    texts_val_tokens.append(text_pipeline_spacy(line))\n",
    "    \n",
    "for line in texts_test_copy:\n",
    "    texts_test_tokens.append(text_pipeline_spacy(line))\n",
    "\n",
    "print(len(texts_train_tokens))\n",
    "print(len(texts_val_tokens))\n",
    "print(len(texts_test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a total vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocabulary(corpus):\n",
    "  vocab = {}\n",
    "  index = 0\n",
    "  for arr in corpus:\n",
    "    for token in arr:\n",
    "      if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "  return vocab\n",
    "\n",
    "vocab = make_vocabulary(texts_train_tokens)\n",
    "print(vocab['pooch'])\n",
    "print(vocab['wallace'])\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the document frequency of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def doc_frequency(corpus):\n",
    "  doc_freq = Counter()\n",
    "  for d in corpus:\n",
    "    unique_tokens = set(d)\n",
    "    for t in unique_tokens:\n",
    "      doc_freq[t] += 1\n",
    "          \n",
    "  return doc_freq\n",
    "\n",
    "training_tokens_doc_frequency = doc_frequency(texts_train_tokens)\n",
    "print(training_tokens_doc_frequency['pooch'])\n",
    "print(training_tokens_doc_frequency['wallace'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a tf-idf sparse vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "\n",
    "def make_tfidf_sparse(tokens, vocab, doc_freq, N):\n",
    "  sparse_vector = {}\n",
    "  counts = Counter(tokens)\n",
    "  for t,c in counts.items():\n",
    "    tf = 1 + m.log10(c) if c > 0 else 0\n",
    "    idf = m.log10(N / doc_freq[t])\n",
    "    sparse_vector[vocab[t]] = tf*idf\n",
    "      \n",
    "  return sparse_vector\n",
    "\n",
    "training_tfidf_sparse = []\n",
    "\n",
    "for text in texts_train_tokens:\n",
    "    training_tfidf_sparse.append(make_tfidf_sparse(text, vocab, training_tokens_doc_frequency, len(texts_train_tokens)))\n",
    "\n",
    "print(training_tfidf_sparse[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sparse_vector(sv):\n",
    "  d = m.sqrt( sum( val*val for index,val in sv.items() ) )\n",
    "  norm_vector = { index:val/d for index,val in sv.items() }\n",
    "  return norm_vector\n",
    "\n",
    "normalised_training_tfidf_sparse = []\n",
    "\n",
    "for vector in training_tfidf_sparse:\n",
    "    normalised_training_tfidf_sparse.append(normalize_sparse_vector(vector))\n",
    "\n",
    "print(normalised_training_tfidf_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sparse tf-idf using sklearn and scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def tfidf_vectorize_with_sklearn_and_spacy(text_corpus):\n",
    "  vectorizer = TfidfVectorizer(tokenizer=text_pipeline_spacy)\n",
    "  X = vectorizer.fit_transform(text_corpus)\n",
    "  return X\n",
    "\n",
    "scipy_sparse_tfidf = tfidf_vectorize_with_sklearn_and_spacy(texts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(scipy_sparse_tfidf))\n",
    "print(scipy_sparse_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going forward I will use the normalised_training_tfidf_sparse\n",
    "\n",
    "Step 1: Pick k random centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def k_random_centroids(tfidf, k):\n",
    "    random.seed(10)\n",
    "    centroids = random.sample(tfidf, k)\n",
    "    return centroids\n",
    "\n",
    "centroids = k_random_centroids(normalised_training_tfidf_sparse, 5)\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of helper functions to be used in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_euclidean_distance(sv1, sv2):\n",
    "  d2 = 0\n",
    "  indices = set(sv1).union(sv2)\n",
    "  for i in indices:\n",
    "    diff = (sv2.get(i,0)) - (sv1.get(i,0))\n",
    "    d2 += diff * diff\n",
    "  return m.sqrt(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_centroid(vectors, centroids):\n",
    "    each_vectors_centroid = []\n",
    "    \n",
    "    for v in vectors:\n",
    "        centroid_distances = -1\n",
    "        current_centroid = 0\n",
    "        closest_centroid = 0\n",
    "        \n",
    "        # Find the closest centroid to that vector\n",
    "        for c in centroids:\n",
    "            distance = sparse_euclidean_distance(v, c)\n",
    "            \n",
    "            # If it is the new closest centroid: set it accordingly\n",
    "            if centroid_distances < 0 or centroid_distances > distance:\n",
    "                centroid_distances = distance\n",
    "                closest_centroid = current_centroid\n",
    "            current_centroid += 1\n",
    "            \n",
    "        # Store closest centroid in same order as vectors  \n",
    "        each_vectors_centroid.append(closest_centroid)\n",
    "    return each_vectors_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters(vectors, each_vectors_centroid, k):\n",
    "    set_of_clusters = [[] for centroid in range(k)]\n",
    "    \n",
    "    for index in range(len(each_vectors_centroid)):\n",
    "        set_of_clusters[each_vectors_centroid[index]].append(vectors[index])\n",
    "        \n",
    "    return set_of_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalculate_centroids(set_of_clusters):\n",
    "    \n",
    "    # Create a set of each centroids summed vectors\n",
    "    set_of_sums = []\n",
    "    for cluster in set_of_clusters:\n",
    "        cluster_sum = {}\n",
    "        for vector in cluster:\n",
    "            for word in vector:\n",
    "                if word in cluster_sum:\n",
    "                    cluster_sum[word] += vector[word]\n",
    "                else:\n",
    "                    cluster_sum[word] = vector[word]\n",
    "        set_of_sums.append(cluster_sum)\n",
    "    \n",
    "    # Get the mean of each cluster, resulting in new centroids\n",
    "    for centroid in set_of_sums:\n",
    "        for word in centroid:\n",
    "            centroid[word] = centroid[word]/len(centroid)\n",
    "    \n",
    "    return set_of_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Calculate clusters for each centroid\n",
    " * Recalculate centroids for each cluster\n",
    " * Recalculate clusters around the new centroids\n",
    " * Repeat until no changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_centroids = get_closest_centroid(normalised_training_tfidf_sparse, centroids)\n",
    "clusters = create_clusters(normalised_training_tfidf_sparse, vector_centroids, 5)\n",
    "change = 1\n",
    "while change != 0:\n",
    "    change = 0\n",
    "    new_centroids = recalculate_centroids(clusters)\n",
    "    each_vectors_centroid = get_closest_centroid(normalised_training_tfidf_sparse, new_centroids)\n",
    "    new_clusters = create_clusters(normalised_training_tfidf_sparse, each_vectors_centroid, 5)\n",
    "    for i in range(5):\n",
    "        change += abs(len(new_clusters[i]) - len(clusters[i]))\n",
    "    clusters = new_clusters\n",
    "    vector_centroids = new_centroids\n",
    "    print(\"change: \", change, \"\\n\")\n",
    "    for i in range(len(clusters)):\n",
    "        print(\"size of cluster \", i,\": \", len(clusters[i]))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vector_centroids))\n",
    "summ = 0\n",
    "for i in clusters:\n",
    "    summ += len(i)\n",
    "    print(len(i))\n",
    "    \n",
    "print(summ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the number of each label in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = [[],[],[],[],[]]\n",
    "for i in range(len(labels_train)):\n",
    "    cluster_labels[each_vectors_centroid[i]].append(labels_train[i])\n",
    "\n",
    "print(\"cluster label count\\n\")\n",
    "index = 0\n",
    "for i in cluster_labels:\n",
    "    label_count = {}\n",
    "    for label in i:\n",
    "        if label in label_count:\n",
    "            label_count[label] += 1\n",
    "        else:\n",
    "            label_count[label] = 1\n",
    "    print(\"cluster\", index)\n",
    "    index += 1\n",
    "    for key,val in label_count.items():\n",
    "        print(key, \": \", val, \" out of \", label_dict_train[key], \" = \", int((val/label_dict_train[key])*100), \"%\", sep='')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the largest tokens in each centroid's cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for vec in vector_centroids:\n",
    "    largest_tokens = []\n",
    "    smallest_token = 0\n",
    "    smallest_token_magnitude = 1\n",
    "    \n",
    "    for tok, mag in vec.items():\n",
    "        \n",
    "        if len(largest_tokens) < 5:\n",
    "            temp = (tok,mag)\n",
    "            largest_tokens.append(temp)\n",
    "            \n",
    "            if mag < smallest_token_magnitude:\n",
    "                smallest_token_magnitude = mag\n",
    "                smallest_token = tok\n",
    "                \n",
    "        elif mag > smallest_token_magnitude:\n",
    "            \n",
    "            for i in range(len(largest_tokens)):\n",
    "                \n",
    "                if largest_tokens[i][0] == smallest_token:\n",
    "                    smallest_token = tok\n",
    "                    smallest_token_magnitude = mag\n",
    "                    largest_tokens[i] = (tok, mag)\n",
    "        \n",
    "    print(\"centroid\", index, \"largest 5 tokens\")\n",
    "    for i in largest_tokens:\n",
    "        word = \"\"\n",
    "        for key, val in vocab.items():\n",
    "            if val == i[0]:\n",
    "                word = key\n",
    "                break\n",
    "        print(word, i[0], i[1])\n",
    "    print(\"\")\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3- Comparing Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import and create dummy classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Dummy classifier = most frequent\n",
    "dummy_clf_mf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf_mf.fit(texts_train, labels_train)\n",
    "labels_predicted_mf = dummy_clf_mf.predict(texts_val)\n",
    "print(\"dummy most frequent score:\", dummy_clf_mf.score(texts_train, labels_train))\n",
    "\n",
    "accuracy_mf = accuracy_score(labels_val, labels_predicted_mf)\n",
    "print(f\"{accuracy_mf=:.3f}\")\n",
    "\n",
    "precision_mf = precision_score(labels_val, labels_predicted_mf, average='macro')\n",
    "print(f\"{precision_mf=:.3f}\")\n",
    "\n",
    "recall_mf = recall_score(labels_val, labels_predicted_mf, average='macro')\n",
    "print(f\"{recall_mf=:.3f}\")\n",
    "\n",
    "f1_mf = f1_score(labels_val, labels_predicted_mf, average='macro')\n",
    "print(f\"{f1_mf=:.3f}\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "# Dummy classifier = stratified\n",
    "dummy_clf_st = DummyClassifier(strategy=\"stratified\", random_state=1)\n",
    "dummy_clf_st.fit(texts_train, labels_train)\n",
    "labels_predicted_st = dummy_clf_st.predict(texts_val)\n",
    "print(\"dummy stratified score:\", dummy_clf_st.score(texts_train, labels_train))\n",
    "\n",
    "accuracy_st = accuracy_score(labels_val, labels_predicted_st)\n",
    "print(f\"{accuracy_st=:.3f}\")\n",
    "\n",
    "precision_st = precision_score(labels_val, labels_predicted_st, average='macro')\n",
    "print(f\"{precision_st=:.3f}\")\n",
    "\n",
    "recall_st = recall_score(labels_val, labels_predicted_st, average='macro')\n",
    "print(f\"{recall_st=:.3f}\")\n",
    "\n",
    "f1_st = f1_score(labels_val, labels_predicted_st, average='macro')\n",
    "print(f\"{f1_st=:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "onehot_vectorizer = CountVectorizer(binary=True)\n",
    "onehot_train = onehot_vectorizer.fit_transform(texts_train)\n",
    "onehot_validate = onehot_vectorizer.transform(texts_val)\n",
    "\n",
    "log_reg_onehot = LogisticRegression(random_state=42)\n",
    "log_reg_onehot.fit(onehot_train,labels_train)\n",
    "labels_predicted_onehot = log_reg_onehot.predict(onehot_validate)\n",
    "\n",
    "accuracy_onehot = accuracy_score(labels_val, labels_predicted_onehot)\n",
    "print(f\"{accuracy_onehot=:.3f}\")\n",
    "\n",
    "precision_onehot = precision_score(labels_val, labels_predicted_onehot, average='macro')\n",
    "print(f\"{precision_onehot=:.3f}\")\n",
    "\n",
    "recall_onehot = recall_score(labels_val, labels_predicted_onehot, average='macro')\n",
    "print(f\"{recall_onehot=:.3f}\")\n",
    "\n",
    "f1_onehot = f1_score(labels_val, labels_predicted_onehot, average='macro')\n",
    "print(f\"{f1_onehot=:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(texts_train)\n",
    "tfidf_validate = tfidf_vectorizer.transform(texts_val)\n",
    "\n",
    "log_reg_tfidf = LogisticRegression(random_state=42)\n",
    "log_reg_tfidf.fit(tfidf_train,labels_train)\n",
    "labels_predicted_tfidf = log_reg_tfidf.predict(tfidf_validate)\n",
    "\n",
    "accuracy_tfidf = accuracy_score(labels_val, labels_predicted_tfidf)\n",
    "print(f\"{accuracy_tfidf=:.3f}\")\n",
    "\n",
    "precision_tfidf = precision_score(labels_val, labels_predicted_tfidf, average='macro')\n",
    "print(f\"{precision_tfidf=:.3f}\")\n",
    "\n",
    "recall_tfidf = recall_score(labels_val, labels_predicted_tfidf, average='macro')\n",
    "print(f\"{recall_tfidf=:.3f}\")\n",
    "\n",
    "f1_tfidf = f1_score(labels_val, labels_predicted_tfidf, average='macro')\n",
    "print(f\"{f1_tfidf=:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC classifier with one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_svc = SVC(kernel='rbf')\n",
    "clf_svc.fit(onehot_train,labels_train)\n",
    "\n",
    "labels_predicted_svc = clf_svc.predict(onehot_validate)\n",
    "\n",
    "accuracy_svc = accuracy_score(labels_val, labels_predicted_svc)\n",
    "print(f\"{accuracy_svc=:.3f}\")\n",
    "\n",
    "precision_svc = precision_score(labels_val, labels_predicted_svc, average='macro')\n",
    "print(f\"{precision_svc=:.3f}\")\n",
    "\n",
    "recall_svc = recall_score(labels_val, labels_predicted_svc, average='macro')\n",
    "print(f\"{recall_svc=:.3f}\")\n",
    "\n",
    "f1_svc = f1_score(labels_val, labels_predicted_svc, average='macro')\n",
    "print(f\"{f1_svc=:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar chart of f1 scores across each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "labels = ['instruction', 'Lady Tottington', 'Piella', 'Victor', 'Wallace']\n",
    "labels_f1 = f1_score(labels_val, labels_predicted_onehot, average=None, labels=labels)\n",
    "print(labels_f1)\n",
    "\n",
    "plt.bar(labels, labels_f1, color='blue')\n",
    "plt.xlabel(\"labels\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.title(\"F1 score for each label\")\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New classifier for part b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc_tfidf = SVC(kernel='rbf')\n",
    "clf_svc_tfidf.fit(tfidf_train,labels_train)\n",
    "\n",
    "labels_predicted_svc_tfidf = clf_svc_tfidf.predict(tfidf_validate)\n",
    "\n",
    "accuracy_svc_tfidf = accuracy_score(labels_val, labels_predicted_svc_tfidf)\n",
    "print(f\"{accuracy_svc_tfidf=:.3f}\")\n",
    "\n",
    "precision_svc_tfidf = precision_score(labels_val, labels_predicted_svc_tfidf, average='macro')\n",
    "print(f\"{precision_svc_tfidf=:.3f}\")\n",
    "\n",
    "recall_svc_tfidf = recall_score(labels_val, labels_predicted_svc_tfidf, average='macro')\n",
    "print(f\"{recall_svc_tfidf=:.3f}\")\n",
    "\n",
    "f1_svc_tfidf = f1_score(labels_val, labels_predicted_svc_tfidf, average='macro')\n",
    "print(f\"{f1_svc_tfidf=:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4- Parameter tuning\n",
    "\n",
    "1. Classifier - Regularistaion C value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_tfidf_1 = LogisticRegression(random_state=42,C=10.0)\n",
    "log_reg_tfidf_1.fit(tfidf_train,labels_train)\n",
    "labels_predicted_tfidf_1 = log_reg_tfidf_1.predict(tfidf_validate)\n",
    "\n",
    "accuracy_tfidf_1 = accuracy_score(labels_val, labels_predicted_tfidf_1)\n",
    "print(f\"{accuracy_tfidf_1=:.3f}\")\n",
    "\n",
    "precision_tfidf_1 = precision_score(labels_val, labels_predicted_tfidf_1, average='macro')\n",
    "print(f\"{precision_tfidf_1=:.3f}\")\n",
    "\n",
    "recall_tfidf_1 = recall_score(labels_val, labels_predicted_tfidf, average='macro')\n",
    "print(f\"{recall_tfidf_1=:.3f}\")\n",
    "\n",
    "f1_tfidf_1 = f1_score(labels_val, labels_predicted_tfidf_1, average='macro')\n",
    "print(f\"{f1_tfidf_1=:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
